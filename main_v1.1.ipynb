{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v1.0 2024-04-26\n",
    "1. 把命令行参数修改为适配jupiter模式，增加一个函数，既可以从命令行获取参数，也可以从jupiter中获取参数\n",
    "2. 把整个程序按照jupiter分段，方便调试\n",
    "\n",
    "v1.1 2024-04-28\n",
    "1. 把数据源改为mysql，训练单支股票"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse # v1.0 用来解析命令行参数\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "from logging.handlers import RotatingFileHandler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import mysql.connector\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入SQL数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_mysql(ts_code): \n",
    "    print (f\"从数据库里获得股票{ts_code}\")\n",
    "\n",
    "    # 连接 MySQL 数据库\n",
    "    engine = create_engine('mysql+mysqlconnector://root:@localhost/stock_ai')\n",
    "    conn = mysql.connector.connect(\n",
    "        host='localhost',\n",
    "        user='root',\n",
    "        password='',\n",
    "        database='stock_ai'\n",
    "    )\n",
    "    if conn.is_connected():\n",
    "        print('连接成功！')\n",
    "        cursor = conn.cursor() # 创建一个游标对象来执行SQL语句\n",
    "    \n",
    "        print(\"开始分析：\",f\"股票{ts_code}\");\n",
    "        select_sub_query = f\"SELECT `low`,`pct_chg`, `day_count`, `open`, `high`, `close`, `average`, `average_vs_pre_close`, `change`, `turnover_rate_f`, `volume_ratio`, `winner`, `pe`, `pb`, `ps`, `dv_ratio`, `k`, `d`, `j`, `macd`, `signal_line`, `macd_histogram`, `ma_5`, `ma_10`, `ma_20`, `ma_60`, `ma_120`, `ma_250`, `rsi` FROM `stock_daily_price` WHERE `ts_code` = '{ts_code}' ORDER BY `trade_date` ASC;\"\n",
    "        # 转换为SQLAlchemy的engine对象\n",
    "        engine = create_engine('mysql+mysqlconnector://root:@localhost/stock_ai')\n",
    "        # 从MySQL数据库中读取数据到DataFrame\n",
    "        df = pd.read_sql_query(select_sub_query, con=engine)\n",
    "    \n",
    "    # 关闭连接\n",
    "    conn.close()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 超参设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "从数据库里获得股票000001\n",
      "连接成功！\n",
      "开始分析： 可转债000001\n"
     ]
    }
   ],
   "source": [
    "frame = \"pytorch\"  # 可选： \"keras\", \"pytorch\", \"tensorflow\"\n",
    "if frame == \"pytorch\":\n",
    "    from model.model_pytorch import train, predict\n",
    "elif frame == \"keras\":\n",
    "    from model.model_keras import train, predict\n",
    "    os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '3'\n",
    "elif frame == \"tensorflow\":\n",
    "    from model.model_tensorflow import train, predict\n",
    "    os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '3'    # tf和keras下会有很多tf的warning，但不影响训练\n",
    "else:\n",
    "    raise Exception(\"Wrong frame seletion\")\n",
    "\n",
    "class Config:\n",
    "    # 数据参数\n",
    "    feature_columns = list(range(0, 29))     # 要作为feature的列，按原数据从0开始计算，也可以用list 如 [2,4,6,8] 设置\n",
    "    label_columns = [0, 1]                  # 要预测的列，按原数据从0开始计算, 如同时预测第四，五列 最低价和最高价\n",
    "    # label_in_feature_index = [feature_columns.index(i) for i in label_columns]  # 这样写不行\n",
    "    label_in_feature_index = (lambda x,y: [x.index(i) for i in y])(feature_columns, label_columns)  # 因为feature不一定从0开始\n",
    "\n",
    "    predict_day = 1             # 预测未来几天\n",
    "\n",
    "    # 网络参数\n",
    "    input_size = len(feature_columns)\n",
    "    output_size = len(label_columns)\n",
    "\n",
    "    hidden_size = 128           # LSTM的隐藏层大小，也是输出大小\n",
    "    lstm_layers = 3             # LSTM的堆叠层数\n",
    "    dropout_rate = 0.2          # dropout概率，示在每次更新时，每个神经元有20%的概率被关闭。防止过度依赖于某个神经元，变成过拟合。\n",
    "    time_step = 30              # 这个参数很重要，是设置用前多少天的数据来预测，也是LSTM的time step数，请保证训练数据量大于它\n",
    "\n",
    "    # 训练参数\n",
    "    do_train = True\n",
    "    do_predict = True\n",
    "    add_train = False           # 是否载入已有模型参数进行增量训练\n",
    "    shuffle_train_data = False   # 是否对训练数据做shuffle，股票的日顺序不应该被打乱。\n",
    "    use_cuda = True            # 是否使用GPU训练\n",
    "\n",
    "    train_data_rate = 0.95      # 训练数据占总体数据比例，测试数据就是 1-train_data_rate\n",
    "    valid_data_rate = 0.15      # 验证数据占训练数据比例，验证集在训练过程使用，为了做模型和参数选择。验证数据是在模型训练过程中用来评估模型性能和调整模型参数的数据，它不参与模型的训练。\n",
    "\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.001\n",
    "    epoch = 20                  # 整个训练集被训练多少遍，不考虑早停的前提下\n",
    "    patience = 5                # 训练多少epoch，验证集没提升就停掉\n",
    "    random_seed = 2024            # 随机种子，保证可复现\n",
    "\n",
    "    do_continue_train = False    # 每次训练把上一次的final_state作为下一次的init_state，仅用于RNN类型模型，目前仅支持pytorch\n",
    "    continue_flag = \"\"           # 但实际效果不佳，可能原因：仅能以 batch_size = 1 训练\n",
    "    if do_continue_train:\n",
    "        shuffle_train_data = False\n",
    "        batch_size = 1\n",
    "        continue_flag = \"continue_\"\n",
    "\n",
    "    # 训练模式\n",
    "    debug_mode = False  # 调试模式下，是为了跑通代码，追求快\n",
    "    debug_num = 500  # 仅用debug_num条数据来调试\n",
    "\n",
    "    # 框架参数\n",
    "    used_frame = frame  # 选择的深度学习框架，不同的框架模型保存后缀不一样\n",
    "    model_postfix = {\"pytorch\": \".pth\", \"keras\": \".h5\", \"tensorflow\": \".ckpt\"}\n",
    "    model_name = \"model_\" + continue_flag + used_frame + model_postfix[used_frame]\n",
    "\n",
    "    # 路径参数\n",
    "    # train_data_path = \"./data/stock_data.csv\"\n",
    "    train_data_path = get_data_from_mysql(\"000001\")\n",
    "    model_save_path = \"./checkpoint/\" + used_frame + \"/\"\n",
    "    figure_save_path = \"./figure/\"\n",
    "    log_save_path = \"./log/\"\n",
    "    do_log_print_to_screen = True\n",
    "    do_log_save_to_file = True                  # 是否将config和训练过程记录到log\n",
    "    do_figure_save = False\n",
    "    do_train_visualized = False          # 训练loss可视化，pytorch用visdom，tf用tensorboardX，实际上可以通用, keras没有\n",
    "    if not os.path.exists(model_save_path):\n",
    "        os.makedirs(model_save_path)    # makedirs 递归创建目录\n",
    "    if not os.path.exists(figure_save_path):\n",
    "        os.mkdir(figure_save_path)\n",
    "    if do_train and (do_log_save_to_file or do_train_visualized):\n",
    "        cur_time = time.strftime(\"%Y_%m_%d_%H_%M_%S\", time.localtime())\n",
    "        log_save_path = log_save_path + cur_time + '_' + used_frame + \"/\"\n",
    "        os.makedirs(log_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练与测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.data, self.data_column_name = self.read_data() # 这个地方要改成从数据库里读取数据\n",
    "\n",
    "        self.data_num = self.data.shape[0]\n",
    "        self.train_num = int(self.data_num * self.config.train_data_rate)\n",
    "\n",
    "        self.mean = np.mean(self.data, axis=0)              # 数据的均值和方差\n",
    "        self.std = np.std(self.data, axis=0)\n",
    "        self.norm_data = (self.data - self.mean)/self.std   # 归一化，去量纲\n",
    "\n",
    "        self.start_num_in_test = 0      # 测试集中前几天的数据会被删掉，因为它不够一个time_step\n",
    "\n",
    "    def read_data(self):                # 读取初始数据\n",
    "        if self.config.debug_mode:\n",
    "            init_data = get_data_from_mysql(\"000001\").head(self.config.debug_num)\n",
    "        else:\n",
    "            init_data = get_data_from_mysql(\"000001\")\n",
    "        return init_data.values, init_data.columns.tolist()     # .columns.tolist() 是获取列名\n",
    "\n",
    "    \"\"\"\n",
    "    def read_data(self):                # 读取初始数据\n",
    "        if self.config.debug_mode:\n",
    "            init_data = pd.read_csv(self.config.train_data_path, nrows=self.config.debug_num,\n",
    "                                    usecols=self.config.feature_columns)\n",
    "        else:\n",
    "            init_data = pd.read_csv(self.config.train_data_path, usecols=self.config.feature_columns)\n",
    "        return init_data.values, init_data.columns.tolist()     # .columns.tolist() 是获取列名\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def get_train_and_valid_data(self):\n",
    "        feature_data = self.norm_data[:self.train_num]\n",
    "        label_data = self.norm_data[self.config.predict_day : self.config.predict_day + self.train_num,\n",
    "                                    self.config.label_in_feature_index]    # 将延后几天的数据作为label\n",
    "\n",
    "        if not self.config.do_continue_train:\n",
    "            # 在非连续训练模式下，每time_step行数据会作为一个样本，两个样本错开一行，比如：1-20行，2-21行。。。。\n",
    "            train_x = [feature_data[i:i+self.config.time_step] for i in range(self.train_num-self.config.time_step)]\n",
    "            train_y = [label_data[i:i+self.config.time_step] for i in range(self.train_num-self.config.time_step)]\n",
    "        else:\n",
    "            # 在连续训练模式下，每time_step行数据会作为一个样本，两个样本错开time_step行，\n",
    "            # 比如：1-20行，21-40行。。。到数据末尾，然后又是 2-21行，22-41行。。。到数据末尾，……\n",
    "            # 这样才可以把上一个样本的final_state作为下一个样本的init_state，而且不能shuffle\n",
    "            # 目前本项目中仅能在pytorch的RNN系列模型中用\n",
    "            train_x = [feature_data[start_index + i*self.config.time_step : start_index + (i+1)*self.config.time_step]\n",
    "                       for start_index in range(self.config.time_step)\n",
    "                       for i in range((self.train_num - start_index) // self.config.time_step)]\n",
    "            train_y = [label_data[start_index + i*self.config.time_step : start_index + (i+1)*self.config.time_step]\n",
    "                       for start_index in range(self.config.time_step)\n",
    "                       for i in range((self.train_num - start_index) // self.config.time_step)]\n",
    "\n",
    "        train_x, train_y = np.array(train_x), np.array(train_y)\n",
    "\n",
    "        train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size=self.config.valid_data_rate,\n",
    "                                                              random_state=self.config.random_seed,\n",
    "                                                              shuffle=self.config.shuffle_train_data)   # 划分训练和验证集，并打乱\n",
    "        return train_x, valid_x, train_y, valid_y\n",
    "\n",
    "    def get_test_data(self, return_label_data=False):\n",
    "        feature_data = self.norm_data[self.train_num:]\n",
    "        sample_interval = min(feature_data.shape[0], self.config.time_step)     # 防止time_step大于测试集数量\n",
    "        self.start_num_in_test = feature_data.shape[0] % sample_interval  # 这些天的数据不够一个sample_interval\n",
    "        time_step_size = feature_data.shape[0] // sample_interval\n",
    "\n",
    "        # 在测试数据中，每time_step行数据会作为一个样本，两个样本错开time_step行\n",
    "        # 比如：1-20行，21-40行。。。到数据末尾。\n",
    "        test_x = [feature_data[self.start_num_in_test+i*sample_interval : self.start_num_in_test+(i+1)*sample_interval]\n",
    "                   for i in range(time_step_size)]\n",
    "        if return_label_data:       # 实际应用中的测试集是没有label数据的\n",
    "            label_data = self.norm_data[self.train_num + self.start_num_in_test:, self.config.label_in_feature_index]\n",
    "            return np.array(test_x), label_data\n",
    "        return np.array(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 记录log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_logger(config):\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(level=logging.DEBUG)\n",
    "\n",
    "    # StreamHandler\n",
    "    if config.do_log_print_to_screen:\n",
    "        stream_handler = logging.StreamHandler(sys.stdout)\n",
    "        stream_handler.setLevel(level=logging.INFO)\n",
    "        formatter = logging.Formatter(datefmt='%Y/%m/%d %H:%M:%S',\n",
    "                                      fmt='[ %(asctime)s ] %(message)s')\n",
    "        stream_handler.setFormatter(formatter)\n",
    "        logger.addHandler(stream_handler)\n",
    "\n",
    "    # FileHandler\n",
    "    if config.do_log_save_to_file:\n",
    "        file_handler = RotatingFileHandler(config.log_save_path + \"out.log\", maxBytes=1024000, backupCount=5)\n",
    "        file_handler.setLevel(level=logging.INFO)\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        file_handler.setFormatter(formatter)\n",
    "        logger.addHandler(file_handler)\n",
    "\n",
    "        # 把config信息也记录到log 文件中\n",
    "        config_dict = {}\n",
    "        for key in dir(config):\n",
    "            if not key.startswith(\"_\"):\n",
    "                config_dict[key] = getattr(config, key)\n",
    "        config_str = str(config_dict)\n",
    "        config_list = config_str[1:-1].split(\", '\")\n",
    "        config_save_str = \"\\nConfig:\\n\" + \"\\n'\".join(config_list)\n",
    "        logger.info(config_save_str)\n",
    "\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算结果绘图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(config: Config, origin_data: Data, logger, predict_norm_data: np.ndarray):\n",
    "    label_data = origin_data.data[origin_data.train_num + origin_data.start_num_in_test : ,\n",
    "                                            config.label_in_feature_index]\n",
    "    predict_data = predict_norm_data * origin_data.std[config.label_in_feature_index] + \\\n",
    "                   origin_data.mean[config.label_in_feature_index]   # 通过保存的均值和方差还原数据\n",
    "    assert label_data.shape[0]==predict_data.shape[0], \"The element number in origin and predicted data is different\"\n",
    "\n",
    "    label_name = [origin_data.data_column_name[i] for i in config.label_in_feature_index]\n",
    "    label_column_num = len(config.label_columns)\n",
    "\n",
    "    # label 和 predict 是错开config.predict_day天的数据的\n",
    "    # 下面是两种norm后的loss的计算方式，结果是一样的，可以简单手推一下\n",
    "    # label_norm_data = origin_data.norm_data[origin_data.train_num + origin_data.start_num_in_test:,\n",
    "    #              config.label_in_feature_index]\n",
    "    # loss_norm = np.mean((label_norm_data[config.predict_day:] - predict_norm_data[:-config.predict_day]) ** 2, axis=0)\n",
    "    # logger.info(\"The mean squared error of stock {} is \".format(label_name) + str(loss_norm))\n",
    "\n",
    "    loss = np.mean((label_data[config.predict_day:] - predict_data[:-config.predict_day] ) ** 2, axis=0)\n",
    "    loss_norm = loss/(origin_data.std[config.label_in_feature_index] ** 2)\n",
    "    logger.info(\"The mean squared error of stock {} is \".format(label_name) + str(loss_norm))\n",
    "\n",
    "    label_X = range(origin_data.data_num - origin_data.train_num - origin_data.start_num_in_test)\n",
    "    predict_X = [ x + config.predict_day for x in label_X]\n",
    "\n",
    "    if not sys.platform.startswith('linux'):    # 无桌面的Linux下无法输出，如果是有桌面的Linux，如Ubuntu，可去掉这一行\n",
    "        for i in range(label_column_num):\n",
    "            plt.figure(i+1)                     # 预测数据绘制\n",
    "            plt.plot(label_X, label_data[:, i], label='label')\n",
    "            plt.plot(predict_X, predict_data[:, i], label='predict')\n",
    "            plt.title(\"Predict stock {} price with {}\".format(label_name[i], config.used_frame))\n",
    "            logger.info(\"The predicted stock {} for the next {} day(s) is: \".format(label_name[i], config.predict_day) +\n",
    "                  str(np.squeeze(predict_data[-config.predict_day:, i])))\n",
    "            if config.do_figure_save:\n",
    "                plt.savefig(config.figure_save_path+\"{}predict_{}_with_{}.png\".format(config.continue_flag, label_name[i], config.used_frame))\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 运行主函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2024/04/29 00:36:22 ] \n",
      "Config:\n",
      "'add_train': False\n",
      "'batch_size': 64\n",
      "'continue_flag': ''\n",
      "'cur_time': '2024_04_29_00_36_22'\n",
      "'debug_mode': False\n",
      "'debug_num': 500\n",
      "'do_continue_train': False\n",
      "'do_figure_save': False\n",
      "'do_log_print_to_screen': True\n",
      "'do_log_save_to_file': True\n",
      "'do_predict': True\n",
      "'do_train': True\n",
      "'do_train_visualized': False\n",
      "'dropout_rate': 0.2\n",
      "'epoch': 20\n",
      "'feature_columns': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]\n",
      "'figure_save_path': './figure/'\n",
      "'hidden_size': 128\n",
      "'input_size': 29\n",
      "'label_columns': [0, 1]\n",
      "'label_in_feature_index': [0, 1]\n",
      "'learning_rate': 0.001\n",
      "'log_save_path': './log/2024_04_29_00_36_22_pytorch/'\n",
      "'lstm_layers': 3\n",
      "'model_name': 'model_pytorch.pth'\n",
      "'model_postfix': {'pytorch': '.pth'\n",
      "'keras': '.h5'\n",
      "'tensorflow': '.ckpt'}\n",
      "'model_save_path': './checkpoint/pytorch/'\n",
      "'output_size': 2\n",
      "'patience': 5\n",
      "'predict_day': 1\n",
      "'random_seed': 2024\n",
      "'shuffle_train_data': False\n",
      "'time_step': 30\n",
      "'train_data_path':        low  pct_chg  day_count   open   high  close  average  \\\n",
      "0    18.44  -3.8263          1  19.10  19.10  18.60    18.61   \n",
      "1    17.80  -2.3118          2  18.40  18.48  18.17    18.03   \n",
      "2    18.00   7.6500          3  18.08  19.56  19.56    18.86   \n",
      "3    19.23   1.7382          4  19.52  19.98  19.90    19.64   \n",
      "4    19.31  -0.2513          5  19.90  20.10  19.85    19.64   \n",
      "..     ...      ...        ...    ...    ...    ...      ...   \n",
      "792  10.04  -1.8537         66  10.22  10.27  10.06    10.13   \n",
      "793  10.06   2.2863         67  10.07  10.32  10.29    10.23   \n",
      "794  10.22  -0.0972         68  10.28  10.39  10.28    10.31   \n",
      "795  10.21   3.3074         69  10.26  10.63  10.62    10.47   \n",
      "796  10.56   1.6949         70  10.58  11.03  10.80    10.83   \n",
      "\n",
      "     average_vs_pre_close  change  turnover_rate_f  ...  macd  signal_line  \\\n",
      "0                0.962254   -0.74           1.8070  ...  0.00         0.00   \n",
      "1                0.969355   -0.43           2.1176  ... -0.03        -0.01   \n",
      "2                1.037970    1.39           2.2496  ...  0.05         0.00   \n",
      "3                1.004090    0.34           1.8418  ...  0.14         0.03   \n",
      "4                0.986935   -0.05           1.3899  ...  0.21         0.07   \n",
      "..                    ...     ...              ...  ...   ...          ...   \n",
      "792              0.988293   -0.19           1.5997  ...  0.00         0.07   \n",
      "793              1.016900    0.23           1.7808  ...  0.00         0.05   \n",
      "794              1.001940   -0.01           1.8112  ... -0.01         0.04   \n",
      "795              1.018480    0.34           2.7359  ...  0.01         0.04   \n",
      "796              1.019770    0.18           3.8796  ...  0.05         0.04   \n",
      "\n",
      "     macd_histogram   ma_5  ma_10  ma_20  ma_60  ma_120  ma_250    rsi  \n",
      "0              0.00  18.60  18.60  18.60  18.60   18.60   18.60    NaN  \n",
      "1             -0.05  18.39  18.39  18.39  18.39   18.39   18.39   0.00  \n",
      "2              0.09  18.78  18.78  18.78  18.78   18.78   18.78  76.37  \n",
      "3              0.22  19.06  19.06  19.06  19.06   19.06   19.06  80.09  \n",
      "4              0.28  19.22  19.22  19.22  19.22   19.22   19.22  78.28  \n",
      "..              ...    ...    ...    ...    ...     ...     ...    ...  \n",
      "792           -0.13  10.28  10.41  10.43  10.04    9.96   10.91  32.77  \n",
      "793           -0.11  10.26  10.39  10.44  10.06    9.95   10.90  47.33  \n",
      "794           -0.10  10.23  10.36  10.42  10.08    9.95   10.90  45.31  \n",
      "795           -0.04  10.30  10.36  10.42  10.11    9.95   10.89  50.70  \n",
      "796            0.02  10.41  10.39  10.44  10.13    9.95   10.88  58.82  \n",
      "\n",
      "[797 rows x 29 columns]\n",
      "'train_data_rate': 0.95\n",
      "'use_cuda': True\n",
      "'used_frame': 'pytorch'\n",
      "'valid_data_rate': 0.15\n",
      "[ 2024/04/29 00:36:22 ] Run Error\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\xzzyc\\AppData\\Local\\Temp\\ipykernel_37300\\1781506312.py\", line 15, in main\n",
      "    data_gainer = Data(config)\n",
      "                  ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xzzyc\\AppData\\Local\\Temp\\ipykernel_37300\\335624705.py\", line 4, in __init__\n",
      "    self.data, self.data_column_name = self.read_data()\n",
      "                                       ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xzzyc\\AppData\\Local\\Temp\\ipykernel_37300\\335624705.py\", line 20, in read_data\n",
      "    init_data = pd.read_csv(self.config.train_data_path, usecols=self.config.feature_columns)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\xzzyc\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py\", line 211, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\xzzyc\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py\", line 331, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\xzzyc\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 950, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\xzzyc\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 605, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\xzzyc\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1442, in __init__\n",
      "    self._engine = self._make_engine(f, self.engine)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\xzzyc\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1735, in _make_engine\n",
      "    self.handles = get_handle(\n",
      "                   ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\xzzyc\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py\", line 704, in get_handle\n",
      "    if _is_binary_mode(path_or_buf, mode) and \"b\" not in mode:\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\xzzyc\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py\", line 1163, in _is_binary_mode\n",
      "    return isinstance(handle, _get_binary_io_classes()) or \"b\" in getattr(\n",
      "                                                           ^^^^^^^^^^^^^^^\n",
      "TypeError: argument of type 'method' is not iterable\n"
     ]
    }
   ],
   "source": [
    "def get_args(): # v1.0 自己写的函数，用来判断是否在notebook环境或者命令行环境，两者获取参数的方式不同。\n",
    "    parser = argparse.ArgumentParser()\n",
    "    if any(\"jupyter\" in arg for arg in sys.argv):\n",
    "        # 如果在Jupyter notebook中运行，手动设置参数\n",
    "        args = parser.parse_args(args=[])\n",
    "    else:\n",
    "        # 如果在命令行中运行，从命令行获取参数\n",
    "        args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "def main(config):\n",
    "    logger = load_logger(config)\n",
    "    try:\n",
    "        np.random.seed(config.random_seed)  # 设置随机种子，保证可复现\n",
    "        data_gainer = Data(config)\n",
    "\n",
    "        if config.do_train:\n",
    "            train_X, valid_X, train_Y, valid_Y = data_gainer.get_train_and_valid_data()\n",
    "            train(config, logger, [train_X, train_Y, valid_X, valid_Y])\n",
    "\n",
    "        if config.do_predict:\n",
    "            test_X, test_Y = data_gainer.get_test_data(return_label_data=True)\n",
    "            pred_result = predict(config, test_X)       # 这里输出的是未还原的归一化预测数据\n",
    "            draw(config, data_gainer, logger, pred_result)\n",
    "    except Exception:\n",
    "        logger.error(\"Run Error\", exc_info=True)\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    import argparse\n",
    "    # argparse方便于命令行下输入参数，可以根据需要增加更多\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # parser.add_argument(\"-t\", \"--do_train\", default=False, type=bool, help=\"whether to train\")\n",
    "    # parser.add_argument(\"-p\", \"--do_predict\", default=True, type=bool, help=\"whether to train\")\n",
    "    # parser.add_argument(\"-b\", \"--batch_size\", default=64, type=int, help=\"batch size\")\n",
    "    # parser.add_argument(\"-e\", \"--epoch\", default=20, type=int, help=\"epochs num\")\n",
    "    \n",
    "    # args = parser.parse_args() v1.0 删除用下面代替\n",
    "    args = get_args()\n",
    "\n",
    "    con = Config()\n",
    "    for key in dir(args):               # dir(args) 函数获得args所有的属性\n",
    "        if not key.startswith(\"_\"):     # 去掉 args 自带属性，比如__name__等\n",
    "            setattr(con, key, getattr(args, key))   # 将属性值赋给Config\n",
    "\n",
    "    main(con)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
